{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended to run on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "# Scikit learn library functionalities\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report                    \n",
    "from sklearn.model_selection import train_test_split           \n",
    "\n",
    "# Tensorflow libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Deep learning models\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, TFAutoModel, BertTokenizer\n",
    "import transformers\n",
    "\n",
    "# For other data manipulations\n",
    "\n",
    "import gc   # Garbage collector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs():\n",
    "    \n",
    "    # Most of the parameters that appear throughout the training process will be kept here\n",
    "    # with some predefined values\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name = \"jplu/tf-xlm-roberta-large\",                                  \n",
    "        max_length = 64,                                # Only important if the param \n",
    "                                                        # pad_to_max_length in the tokenizer is\n",
    "                                                        # set to True. \n",
    "        batch_size = 16,                                # Samples per batch\n",
    "        epochs = 20,                                    # Times to go through the data\n",
    "        accelerator = \"TPU\"                             # Preferred since we work on Kaggle\n",
    "    ):\n",
    "    \n",
    "        self.ACCELERATOR = accelerator\n",
    "        self.MODEL_NAME = model_name\n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_NAME)\n",
    "        self.MAX_LENGTH = max_length\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.EPOCHS = epochs\n",
    "        \n",
    "        self.initialize_accelerator()\n",
    "\n",
    "    # Since we are using Kaggle for TPUs, the following code is required in order to run\n",
    "    # our models faster\n",
    "    \n",
    "    def initialize_accelerator(self):\n",
    "        if self.ACCELERATOR == \"TPU\":\n",
    "            try:\n",
    "                tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "                print(f\"Running on TPU {tpu.master()}\")\n",
    "            except ValueError:\n",
    "                print(\"Could not connect to TPU\")\n",
    "                tpu = None\n",
    "            if tpu:\n",
    "                try:\n",
    "                    print(\"Initializing TPU\")\n",
    "                    tf.config.experimental_connect_to_cluster(tpu)\n",
    "                    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "                    self.strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "                    self.tpu = tpu\n",
    "                    print(\"TPU initialized\")\n",
    "                except _:\n",
    "                    print(\"Failed to initialize TPU\")\n",
    "            else:\n",
    "                print(\"Unable to initialize TPU\")\n",
    "                self.ACCELERATOR = \"GPU\"\n",
    "\n",
    "        # In case TPU is not available it checks for GPU, if none are available it \n",
    "        # continues with CPU\n",
    "        \n",
    "        if self.ACCELERATOR != \"TPU\":\n",
    "            print(\"Using default strategy for CPU and single GPU\")\n",
    "            self.strategy = tf.distribute.get_strategy()\n",
    "\n",
    "        if self.ACCELERATOR == \"GPU\":\n",
    "            \n",
    "            print(f\"GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "\n",
    "        self.AUTO = tf.data.experimental.AUTOTUNE\n",
    "        self.REPLICAS = self.strategy.num_replicas_in_sync\n",
    "        print(f\"REPLICAS: {self.REPLICAS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the sentences \n",
    "\n",
    "def encode(data, tokenizer, max_length):\n",
    "    \n",
    "    text = data.values.tolist()   # Receives the two columns with the premise and hypothesis\n",
    "\n",
    "    # The model has its own tokenizer so we use it (AutoTokenizer or BertTokenizer)\n",
    "    # pad_to_max length is set to True so that all samples will be of same length\n",
    "    \n",
    "    encoded = tokenizer.batch_encode_plus(text, pad_to_max_length = True, max_length = max_length)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(X, y, auto, label = True, repeat = False, shuffle = False, batch_size = 128):\n",
    "    \n",
    "    if label == True:\n",
    "        data = (tf.data.Dataset.from_tensor_slices((X[\"input_ids\"], y)))\n",
    "    else:\n",
    "        data = (tf.data.Dataset.from_tensor_slices(X[\"input_ids\"]))\n",
    "\n",
    "    if repeat:\n",
    "        data = data.repeat()\n",
    "\n",
    "    if shuffle:\n",
    "        data = data.shuffle(2048)\n",
    "\n",
    "    data = data.batch(batch_size)\n",
    "    data = data.prefetch(auto)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the train history for the keras model\n",
    "\n",
    "def show_train_history(train_history,train,validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "def build_model(model_name, max_length):\n",
    "    \n",
    "    # Define encoded inputs\n",
    "    input_ids = Input(shape = (max_length,), dtype = tf.int32, name = \"input_ids\")    \n",
    "    \n",
    "    # Define transformer model embeddings\n",
    "    transformer_model = TFAutoModel.from_pretrained(model_name)\n",
    "    transformer_embeddings = transformer_model(input_ids)[0]\n",
    "    \n",
    "    # Define the output layer, as a dense layer with softmax activation function\n",
    "    output_values = Dense(3, activation = \"softmax\")(transformer_embeddings[:, 0, :])\n",
    "\n",
    "    # We define the model with the input and output values we defined previously\n",
    "    # The model is pretrained, except for the output layer\n",
    "    \n",
    "    model = Model(inputs = input_ids, outputs = output_values)        \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "    model.compile(optimizer = Adam(learning_rate = 1e-5), loss = loss, metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "\n",
    "def run_model(train, config):\n",
    "\n",
    "    \"\"\"Reads the train and test sets and the class withe the configurations\"\"\"\n",
    "    \n",
    "    # First we initialize the accelerator in the config class\n",
    "    \n",
    "    if config.ACCELERATOR == \"TPU\":\n",
    "        if config.tpu:\n",
    "            config.initialize_accelerator()\n",
    "\n",
    "    # Then we build the model under TPU, the model needs to only be built under TPU, when \n",
    "    # we train it it knows with which accelerator it should work\n",
    "    \n",
    "    K.clear_session()\n",
    "    with config.strategy.scope():\n",
    "        model = build_model(config.MODEL_NAME, config.MAX_LENGTH)\n",
    "        print(model.summary())\n",
    "\n",
    "    # Splitting data into training and validation sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train[[\"premise\",\"hypothesis\"]], train[\"label\"], test_size=0.25, random_state=42)\n",
    "    \n",
    "    print(\"\\nTokenizing\")\n",
    "\n",
    "    # Encoding text data using tokenizer\n",
    "    tokenizer = config.TOKENIZER\n",
    "    X_train_encoded = encode(X_train, tokenizer, config.MAX_LENGTH)\n",
    "    X_test_encoded = encode(X_test, tokenizer, config.MAX_LENGTH)\n",
    "\n",
    "    # Creating TF Dataset, because we can't work with the encodings format outputed from \n",
    "    # the encoding function\n",
    "    # The batch size we pass is defined as the predefined batch size times the num of tpu\n",
    "    # replicas found. This practice is recommended by Kaggle\n",
    "    \n",
    "    train_data = to_dataset(X_train_encoded, y_train, config.AUTO, repeat = True, shuffle = True, batch_size = config.BATCH_SIZE * config.REPLICAS)\n",
    "    test_data = to_dataset(X_test_encoded, y_test, config.AUTO, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n",
    "\n",
    "    n_train = X_train.shape[0]\n",
    "   \n",
    "    # Saving model at best accuracy epoch\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"model.h5\",\n",
    "        monitor = \"val_accuracy\",\n",
    "        verbose = 0,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "        mode = \"max\",\n",
    "        save_freq = \"epoch\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nTraining\")\n",
    "\n",
    "    # Training model\n",
    "    model_history = model.fit(\n",
    "        train_data,\n",
    "        epochs = config.EPOCHS,\n",
    "        callbacks = [sv],\n",
    "        steps_per_epoch = n_train / config.BATCH_SIZE // config.REPLICAS,\n",
    "        validation_data = test_data,\n",
    "        verbose = 0\n",
    "    )\n",
    "    \n",
    "    # Visualize the loss and accuracy after each epoch\n",
    "    show_train_history(model_history,'accuracy','val_accuracy')\n",
    "    show_train_history(model_history,'loss','val_loss')\n",
    "\n",
    "    print(\"\\nValidating\")\n",
    "\n",
    "    # scoring validation data\n",
    "    model.load_weights(\"model.h5\")\n",
    "    test_data = to_dataset(X_test_encoded, -1, config.AUTO, label = False, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n",
    "\n",
    "    preds_test = model.predict(test_data, verbose = 0)\n",
    "    acc = accuracy_score(y_test, np.argmax(preds_test, axis = 1))\n",
    "\n",
    "    print(f\"\\n Accuracy: {round(acc, 4)}\\n\")\n",
    "\n",
    "    g = gc.collect()\n",
    "\n",
    "    return preds_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading data\n",
    "train = pd.read_csv(\"../input/nli-sentences/translated_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU grpc://10.0.0.2:8470\n",
      "Initializing TPU\n"
     ]
    }
   ],
   "source": [
    "config = Configs(max_length = 64, batch_size = 64)\n",
    "preds_test, y_test = run_model(train,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(np.argmax(preds_test, axis = 1), y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
